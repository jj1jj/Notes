
- **预处理和数据增强技术**

> https://zhpmatrix.github.io/2019/03/08/preprocess-augmentation-in-nlp/

- **nlp实践技巧**

> http://ruder.io/deep-learning-nlp-best-practices/index.html

- **多任务学习**

> http://ruder.io/multi-task/index.html

- **Non-Zero initial for CNN**

> https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html

- **依存分析tutorial**

> https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lecture_notes/cs224n-2017-notes4.pdf

- **依存分析评价指标**

> https://jiaxuncai.github.io/2016/10/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/

- **good understanding of lstm**

> http://colah.github.io/posts/2015-08-Understanding-LSTMs/

- **transformer 详解**

> http://jalammar.github.io/illustrated-transformer/

- **数据并行 多GPU**

> https://github.com/zergtant/pytorch-handbook/blob/b572c428af9008f36517b799202245bdcf7bc70c/chapter1/5_data_parallel_tutorial.ipynb

- **"Attention is all you need" transformer pytorch实现**

> http://nlp.seas.harvard.edu/2018/04/03/attention.html

- **Transformer 模型的 PyTorch 实现**

> https://juejin.im/post/5b9f1af0e51d450e425eb32d#comment

- **Seq2Seq with attention**

> https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

- **RNN bptt 详解**

> http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

- **Why in pytorch's examples detach hidden in RNN from every batch?**

> https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/9

- **各种attention**

> https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

- **CRF 深刻理解**

> http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/

- **CRF (数学原理）**

> http://www.cs.columbia.edu/~mcollins/crf.pdf

- **通俗理解（有实例）NER中lstm&crf的算法流程**

> https://zhuanlan.zhihu.com/p/44042528

- **通俗理解（有实例）维特比算法**

> https://www.zhihu.com/question/20136144/answer/239971177

- **pytorch model_summary **

> https://github.com/sksq96/pytorch-summary

- **pytorch trick**

> https://zhuanlan.zhihu.com/p/76459295

- **pytorch 版本更新**

> http://www.ishenping.com/ArtInfo/50913.html

- **文本分类**

> https://www.jiqizhixin.com/articles/2018-10-29-10

- **torchtext 入门**

> http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/

- **torchtext详解**

> https://blog.csdn.net/u012436149/article/details/79310176

- **PCA whitening**

> http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/

- 苏剑林 **attention is all you need **

> https://kexue.fm/archives/4765

- **Gensim-fasttext 上岸教程**

> https://blog.csdn.net/sinat_26917383/article/details/83041424

- **Word2vec Gensim 初级教程**

> https://www.jianshu.com/p/ce630c198762

- **tmux 个人最初配置参考**

> https://zhuanlan.zhihu.com/p/58668651

- **Deep Biaffine Attention Neural Dependency Parsing 导读**

> https://xiaoxiaoaurora.github.io/2019/04/11/Deep-Biaffine-Attention-for-Neural-Dependency-Parsing/

