

- **预处理和数据增强技术**

> https://zhpmatrix.github.io/2019/03/08/preprocess-augmentation-in-nlp/

- **nlp实践技巧**

> http://ruder.io/deep-learning-nlp-best-practices/index.html

- **多任务学习**

> http://ruder.io/multi-task/index.html

- **Non-Zero initial for CNN**

> https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html

- **依存分析tutorial**

> https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lecture_notes/cs224n-2017-notes4.pdf

- **依存分析评价指标**

> https://jiaxuncai.github.io/2016/10/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/

- **good understanding of lstm**

> http://colah.github.io/posts/2015-08-Understanding-LSTMs/

- **transformer 详解**

> http://jalammar.github.io/illustrated-transformer/

- **数据并行 多GPU**

> https://github.com/zergtant/pytorch-handbook/blob/b572c428af9008f36517b799202245bdcf7bc70c/chapter1/5_data_parallel_tutorial.ipynb

- **"Attention is all you need" transformer pytorch实现**

> http://nlp.seas.harvard.edu/2018/04/03/attention.html

- **Seq2Seq with attention**

> https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

- **RNN bptt 详解**

> http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

- **Why in pytorch's examples detach hidden in RNN from every batch?**

> https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/9