
- **预处理和数据增强技术**

> https://zhpmatrix.github.io/2019/03/08/preprocess-augmentation-in-nlp/

- **nlp实践技巧**

> http://ruder.io/deep-learning-nlp-best-practices/index.html

- **多任务学习**

> http://ruder.io/multi-task/index.html

- **Non-Zero initial for CNN**

> https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html

- **依存分析tutorial**

> https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lecture_notes/cs224n-2017-notes4.pdf

- **依存分析评价指标**

> https://jiaxuncai.github.io/2016/10/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/

- **good understanding of lstm**

> http://colah.github.io/posts/2015-08-Understanding-LSTMs/

- **transformer 详解**

> http://jalammar.github.io/illustrated-transformer/

- **数据并行 多GPU**

> https://github.com/zergtant/pytorch-handbook/blob/b572c428af9008f36517b799202245bdcf7bc70c/chapter1/5_data_parallel_tutorial.ipynb

- **"Attention is all you need" transformer pytorch实现**

> http://nlp.seas.harvard.edu/2018/04/03/attention.html

- **Transformer 模型的 PyTorch 实现**

> https://juejin.im/post/5b9f1af0e51d450e425eb32d#comment

- **Seq2Seq with attention**

> https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

- **RNN bptt 详解**

> http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

- **Why in pytorch's examples detach hidden in RNN from every batch?**

> https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/9

- **各种attention**

> https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

- **CRF 深刻理解**

> http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/

- **CRF (数学原理）**

> http://www.cs.columbia.edu/~mcollins/crf.pdf

- **通俗理解（有实例）NER中lstm&crf的算法流程**

> https://zhuanlan.zhihu.com/p/44042528

- **通俗理解（有实例）维特比算法**

> https://www.zhihu.com/question/20136144/answer/239971177

- **pytorch model_summary **

> https://github.com/sksq96/pytorch-summary

- **pytorch trick**

> https://zhuanlan.zhihu.com/p/76459295

- **pytorch 版本更新**

> http://www.ishenping.com/ArtInfo/50913.html

- **文本分类**

> https://www.jiqizhixin.com/articles/2018-10-29-10

- **torchtext 入门**

> http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/

- **torchtext详解**

> https://blog.csdn.net/u012436149/article/details/79310176

- **PCA whitening**

> http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/

- 苏剑林 **attention is all you need **

> https://kexue.fm/archives/4765

- **Gensim-fasttext 上岸教程**

> https://blog.csdn.net/sinat_26917383/article/details/83041424

- **Word2vec Gensim 初级教程**

> https://www.jianshu.com/p/ce630c198762

- **tmux 个人最初配置参考**

> https://zhuanlan.zhihu.com/p/58668651

- **Deep Biaffine Attention Neural Dependency Parsing 导读**

> http://www.hankcs.com/nlp/parsing/deep-biaffine-attention-for-neural-dependency-parsing.html

- **vscode 免密登陆最简单的配置**

> https://www.cnblogs.com/dabai56/p/11057127.html

- **bert入门**

> https://evilpsycho.github.io/2019/05/16/Kaggle-Jigsaw-%E6%81%B6%E6%84%8F%E8%AF%84%E8%AE%BA%E8%AF%86%E5%88%AB-bert/#more

- **学习率调整函数 pytorch**

> https://zhuanlan.zhihu.com/p/69411064

- **PyTorch Cookbook（常用代码段整理合集）**

> https://zhuanlan.zhihu.com/p/59205847

- **linux 命令tutorial**

> https://www.runoob.com/linux/linux-tutorial.html

- **ipdb调试**

> https://michael728.github.io/2018/12/15/python-debug-ipdb/

- **word2vec的数学原理 硬核**

> https://www.cnblogs.com/peghoty/p/3857839.html

- **fasttext c++源码剖析 硬核**

> https://heleifz.github.io/14732610572844.html